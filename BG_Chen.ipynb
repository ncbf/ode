{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 and x2 are sequences generated from constrained random walks.  x1 (x2) changes with a prob per unit time of \n",
    "0.65 (0.99) by a random amount uniformly distributed between -0.5 and 0.5 (-1 and 1). The modulus of x is then \n",
    "taken to ensure a positive sequence. Initial v are randomly selected from a unif fistrbution between -1 and 1. \n",
    "\n",
    "v1 ans v2 are unif randomly selected between -1 and +1.\n",
    "\n",
    "100 temporal patterns are simulated numerically with different sequences for x1 and x2, and different inital v1 and v2. Tau goes from 0.5 to 0.8 with dt=0.1\n",
    "\n",
    "Network: 8 hidden nodes and trained on 50 of the processes. Results shown on 50 of the processes not seen during the training. Set alpha=0.01 and beta=40 000. \n",
    "\n",
    "Temporak pattern: network produces sequence v(1) ... v(T) given initial conditions v(0) and ext input x(0), ..., x(T-1) and time steps. \n",
    "\n",
    "The neural network takes as input v(i) and x(i), and computes the function dv/dt=F(v,x).\n",
    "First layer: inputs v(i) and x(i)\n",
    "Second layer: 8 hidden nodes with tanh function \n",
    "Output layr: dv/dt no activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The diff equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_equ(t, V, x1, x2):\n",
    "    #x1, x2 =tf.split(X[tf.cast(t, tf.int32)],1)\n",
    "    v1, v2 = V \n",
    "    dv1= x1[tf.dtypes.cast(tf.round(t), tf.int32)] -2*v1 + 8*v2 -x1[tf.dtypes.cast(tf.round(t), tf.int32)]*v1\n",
    "    dv2= x2[tf.dtypes.cast(tf.round(t), tf.int32)] -5*v1 + v2 -x2[tf.dtypes.cast(tf.round(t), tf.int32)]*v2\n",
    "    dV=np.array([dv1, dv2])\n",
    "    return dV   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate x value sequences for 0... T-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_x(T):\n",
    "    #initial values \n",
    "    x1=np.absolute(np.array([np.random.uniform(low=-0.5, high=0.5)]))\n",
    "    x2=np.absolute(np.array([np.random.uniform(low=-1.0, high=1.0)]))\n",
    "    #x1=np.absolute(np.array([[np.random.uniform(low=-0.5, high=0.5)]]))\n",
    "    #x2=np.absolute(np.array([[np.random.uniform(low=-1.0, high=1.0)]]))\n",
    "    for i in range(T): \n",
    "        #generate a bernouilli according to PD\n",
    "        p1 = np.random.binomial(1,p=0.65)\n",
    "        p2 = np.random.binomial(1,p=0.99)\n",
    "        u1=np.absolute(np.array([np.random.uniform(low=-0.5, high=0.5)]))\n",
    "        u2=np.absolute(np.array([np.random.uniform(low=-1.0, high=1.0)]))\n",
    "        x1=np.append(x1, x1[-1]+p1*u1, axis=0)\n",
    "        x2=np.append(x2, x2[-1]+p2*u2, axis=0)\n",
    "    #X=np.hstack((x1,x2))\n",
    "    return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularisation para see equ.(24)\n",
    "alpha=0.01\n",
    "#Loss function paraÂ´see equ.(10)\n",
    "beta=40000\n",
    "\n",
    "ODESolver = tfp.math.ode.DormandPrince()\n",
    "\n",
    "# initial conditions and time grid\n",
    "t_initial = 0 # initial time\n",
    "t_final = 8 # final time\n",
    "n_times = 80 # number of time steps to churn out solution for\n",
    "times = np.linspace(t_initial, t_final, n_times).astype(np.float32) \n",
    "\n",
    "#v1=np.array([[np.random.uniform(low=-1.0, high=1.0)]])\n",
    "#v2=np.array([[np.random.uniform(low=-1.0, high=1.0)]])\n",
    "#v_initial = np.array([v1, v2]).astype(np.float32) # initial state vector\n",
    "\n",
    "v1=np.random.uniform(low=-1.0, high=1.0)\n",
    "v2=np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "v_initial = np.array([v1, v2]).astype(np.float32)\n",
    "\n",
    "x1, x2=gen_x(n_times)\n",
    "# integrate the ODE\n",
    "results = ODESolver.solve(diff_equ, # system of ODEs (gradient function)\n",
    "                                   t_initial, # initial time\n",
    "                                   v_initial, # initial state\n",
    "                                   solution_times=times,\n",
    "                                   constants={'x1': x1.astype(np.float32),'x2': x2.astype(np.float32)}) # time grid to spit out solutions for\n",
    "\n",
    "# extract results for the state solutions v(t)\n",
    "data = tf.cast(tf.stack(results.states, axis=-1), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67015994, 0.43875894], dtype=float32)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=0.1\n",
    "def euler(v, dv):\n",
    "    dv1, dv2=dv\n",
    "    new_v1=v1+dv1*dt\n",
    "    new_v2=v2+dv2*dt\n",
    "    V=np.array([new_v1, new_v2])\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases:\n",
    "n_state = 2\n",
    "n_ext=2\n",
    "n_hidden = 8\n",
    "W1_v = tf.Variable(tf.random.normal([(n_state), n_hidden], 0, 0.1),  trainable=True)\n",
    "W1_x=  tf.Variable(tf.random.normal([(n_ext), n_hidden], 0, 0.1),  trainable=True)\n",
    "b1 = tf.Variable(tf.random.normal([n_hidden], 0, 0.1),  trainable=True)\n",
    "W2 = tf.Variable(tf.random.normal([n_hidden, n_state], 0, 0.1),  trainable=True)\n",
    "b2 = tf.Variable(tf.random.normal([n_state], 0, 0.1),  trainable=True)\n",
    "\n",
    "def dvdt_nn(t,v,x):\n",
    "    output=tf.matmul(tf.expand_dims(v, axis=0), W1_v)+tf.matmul(tf.expand_dims(x, axis=0), W1_x)+ b1\n",
    "    dvdt = tf.matmul(tf.math.tanh(output), W2) + b2   \n",
    "    return tf.squeeze(dvdt, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given epoch: \n",
    "    Error: e=v-T where T is the target \n",
    "    Total error: 1/2 \\sum \\beta(e)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BJ step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.10277498, -0.14624238], dtype=float32)>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array([x1,x2]).astype(np.float32)\n",
    "x=np.transpose(x)\n",
    "\n",
    "dvdt_nn(0, v_initial,x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6599839 , 0.4241022 ],\n",
       "       [0.6570614 , 0.42263138],\n",
       "       [0.6545524 , 0.42119375],\n",
       "       [0.6534703 , 0.42052692],\n",
       "       [0.6512192 , 0.41888776],\n",
       "       [0.65092736, 0.41809377],\n",
       "       [0.6496218 , 0.4174395 ],\n",
       "       [0.64859146, 0.41655445],\n",
       "       [0.64670265, 0.41532183],\n",
       "       [0.6466064 , 0.41527537],\n",
       "       [0.64531493, 0.41466442],\n",
       "       [0.6432799 , 0.41371065],\n",
       "       [0.6421506 , 0.4131909 ],\n",
       "       [0.641802  , 0.4126795 ],\n",
       "       [0.6407539 , 0.41201898],\n",
       "       [0.64037704, 0.41183418],\n",
       "       [0.6408523 , 0.41165274],\n",
       "       [0.6401087 , 0.41131696],\n",
       "       [0.64009535, 0.4113115 ],\n",
       "       [0.640496  , 0.41120204],\n",
       "       [0.64051116, 0.4110791 ],\n",
       "       [0.63995975, 0.4108433 ],\n",
       "       [0.6393891 , 0.41059983],\n",
       "       [0.6390486 , 0.41039857],\n",
       "       [0.638342  , 0.41009718],\n",
       "       [0.63836515, 0.40982828],\n",
       "       [0.63854074, 0.4096697 ],\n",
       "       [0.6378877 , 0.4091116 ],\n",
       "       [0.6378624 , 0.40881482],\n",
       "       [0.637594  , 0.4085532 ],\n",
       "       [0.6368435 , 0.408236  ],\n",
       "       [0.63672286, 0.40818867],\n",
       "       [0.6360886 , 0.40778702],\n",
       "       [0.63562876, 0.40760535],\n",
       "       [0.63503116, 0.40736777],\n",
       "       [0.6349326 , 0.4073243 ],\n",
       "       [0.6345804 , 0.4071519 ],\n",
       "       [0.6344813 , 0.4069947 ],\n",
       "       [0.63422024, 0.40680233],\n",
       "       [0.6338378 , 0.40665197],\n",
       "       [0.63347584, 0.406412  ],\n",
       "       [0.63329077, 0.40632135],\n",
       "       [0.63301075, 0.40616387],\n",
       "       [0.6328676 , 0.40610835],\n",
       "       [0.6326614 , 0.40602812],\n",
       "       [0.63231474, 0.40589237],\n",
       "       [0.6319662 , 0.40575483],\n",
       "       [0.6317947 , 0.40568203],\n",
       "       [0.63146484, 0.4055503 ],\n",
       "       [0.63140535, 0.40546098],\n",
       "       [0.63110894, 0.40534514],\n",
       "       [0.63100064, 0.40528202],\n",
       "       [0.6308642 , 0.40522912],\n",
       "       [0.6306867 , 0.4051211 ],\n",
       "       [0.63049114, 0.4050265 ],\n",
       "       [0.6302915 , 0.40494144],\n",
       "       [0.63021016, 0.4049095 ],\n",
       "       [0.6300716 , 0.40486047],\n",
       "       [0.6298903 , 0.40479866],\n",
       "       [0.6297827 , 0.4047616 ],\n",
       "       [0.6297434 , 0.404759  ],\n",
       "       [0.6296058 , 0.40472504],\n",
       "       [0.62943   , 0.40469214],\n",
       "       [0.62926424, 0.40465257],\n",
       "       [0.62914014, 0.40461528],\n",
       "       [0.62902063, 0.40457854],\n",
       "       [0.62886846, 0.40453136],\n",
       "       [0.628818  , 0.40452227],\n",
       "       [0.6287087 , 0.40448898],\n",
       "       [0.6286461 , 0.4044701 ],\n",
       "       [0.62852675, 0.4044343 ],\n",
       "       [0.62845707, 0.40442425],\n",
       "       [0.62839586, 0.40442613],\n",
       "       [0.62824374, 0.40441507],\n",
       "       [0.62807417, 0.4044109 ],\n",
       "       [0.62802017, 0.40439615],\n",
       "       [0.62794536, 0.40439603],\n",
       "       [0.6278617 , 0.40439248],\n",
       "       [0.6278019 , 0.4043977 ],\n",
       "       [0.62765974, 0.40442514]], dtype=float32)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 1e-3)\n",
    "\n",
    "# training step: computes predictions, compute MSE between predictions and data, computes gradients and applies them to the parameters\n",
    "def training_step():\n",
    "    \n",
    "    # start the gradient tape: this records all operations in a way that gradients can be taken\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions=[]\n",
    "        for i in range(len(times)):\n",
    "            net=dvdt_nn(times[i], v, x[i])\n",
    "            pred_v = euler(v, net)\n",
    "            predictions.append(pred_v)\n",
    "        predictions=np.asarray(predictions)\n",
    "        \n",
    "        # calculate loss\n",
    "        error=data-predictions \n",
    "        loss=0.5*beta*np.linalg.norm(error)**2       \n",
    "        \n",
    "    # calculate gradients\n",
    "    gradients = tape.gradient(loss, tape.watched_variables())\n",
    "    \n",
    "    # make gradient step\n",
    "    optimizer.apply_gradients(zip(gradients, tape.watched_variables()))   \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2636175764fc4af5b72696242b4699c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimizing', max=200.0, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [2,80] vs. [80,2] [Op:Sub]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-284-e4a2a9718302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-283-dce8af78c31f>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m  10454\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10455\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10456\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10457\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10458\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [2,80] vs. [80,2] [Op:Sub]"
     ]
    }
   ],
   "source": [
    "# list to keep the loss values for plotting later (to see if it's converged)\n",
    "losses = []\n",
    "\n",
    "# number of trainine epochs\n",
    "n_epochs = 200\n",
    "\n",
    "# progressbar\n",
    "pbar = tqdm.tqdm_notebook(total = n_epochs, desc = \"Optimizing\")\n",
    "pbar.set_postfix(ordered_dict={\"loss\":None}, refresh=True)\n",
    "\n",
    "# train it! loop over epochs, doing a gradient update each step\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    loss = training_step()\n",
    "    losses.append(loss.numpy())\n",
    "    \n",
    "    # update progressbar\n",
    "    pbar.update()\n",
    "    pbar.set_postfix(ordered_dict={\"loss\":loss.numpy()}, refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
